{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/PRNU/data\n"
     ]
    }
   ],
   "source": [
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.engine import InputSpec\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Input, Conv2D, Activation, BatchNormalization\n",
    "from keras.layers.merge import Add\n",
    "from keras.utils import conv_utils\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "\n",
    "def res_block(input, filters, kernel_size=(3, 3), strides=(1, 1), use_dropout=False):\n",
    "    \"\"\"\n",
    "    Instanciate a Keras Resnet Block using sequential API.\n",
    "    :param input: Input tensor\n",
    "    :param filters: Number of filters to use\n",
    "    :param kernel_size: Shape of the kernel for the convolution\n",
    "    :param strides: Shape of the strides for the convolution\n",
    "    :param use_dropout: Boolean value to determine the use of dropout\n",
    "    :return: Keras Model\n",
    "    \"\"\"\n",
    "    x = ReflectionPadding2D((1, 1))(input)\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               strides=strides,)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    if use_dropout:\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    x = ReflectionPadding2D((1, 1))(x)\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               strides=strides,)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    merged = Add()([input, x])\n",
    "    return merged\n",
    "\n",
    "\n",
    "def spatial_reflection_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n",
    "    \"\"\"\n",
    "    Pad the 2nd and 3rd dimensions of a 4D tensor.\n",
    "    :param x: Input tensor\n",
    "    :param padding: Shape of padding to use\n",
    "    :param data_format: Tensorflow vs Theano convention ('channels_last', 'channels_first')\n",
    "    :return: Tensorflow tensor\n",
    "    \"\"\"\n",
    "    assert len(padding) == 2\n",
    "    assert len(padding[0]) == 2\n",
    "    assert len(padding[1]) == 2\n",
    "    if data_format is None:\n",
    "        data_format = image_data_format()\n",
    "    if data_format not in {'channels_first', 'channels_last'}:\n",
    "        raise ValueError('Unknown data_format ' + str(data_format))\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        pattern = [[0, 0],\n",
    "                   [0, 0],\n",
    "                   list(padding[0]),\n",
    "                   list(padding[1])]\n",
    "    else:\n",
    "        pattern = [[0, 0],\n",
    "                   list(padding[0]), list(padding[1]),\n",
    "                   [0, 0]]\n",
    "    return tf.pad(x, pattern, \"REFLECT\")\n",
    "\n",
    "\n",
    "# TODO: Credits\n",
    "class ReflectionPadding2D(Layer):\n",
    "    \"\"\"Reflection-padding layer for 2D input (e.g. picture).\n",
    "    This layer can add rows and columns or zeros\n",
    "    at the top, bottom, left and right side of an image tensor.\n",
    "    # Arguments\n",
    "        padding: int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n",
    "            - If int: the same symmetric padding\n",
    "                is applied to width and height.\n",
    "            - If tuple of 2 ints:\n",
    "                interpreted as two different\n",
    "                symmetric padding values for height and width:\n",
    "                `(symmetric_height_pad, symmetric_width_pad)`.\n",
    "            - If tuple of 2 tuples of 2 ints:\n",
    "                interpreted as\n",
    "                `((top_pad, bottom_pad), (left_pad, right_pad))`\n",
    "        data_format: A string,\n",
    "            one of `channels_last` (default) or `channels_first`.\n",
    "            The ordering of the dimensions in the inputs.\n",
    "            `channels_last` corresponds to inputs with shape\n",
    "            `(batch, height, width, channels)` while `channels_first`\n",
    "            corresponds to inputs with shape\n",
    "            `(batch, channels, height, width)`.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"channels_last\".\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        - If `data_format` is `\"channels_last\"`:\n",
    "            `(batch, rows, cols, channels)`\n",
    "        - If `data_format` is `\"channels_first\"`:\n",
    "            `(batch, channels, rows, cols)`\n",
    "    # Output shape\n",
    "        4D tensor with shape:\n",
    "        - If `data_format` is `\"channels_last\"`:\n",
    "            `(batch, padded_rows, padded_cols, channels)`\n",
    "        - If `data_format` is `\"channels_first\"`:\n",
    "            `(batch, channels, padded_rows, padded_cols)`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 padding=(1, 1),\n",
    "                 data_format=None,\n",
    "                 **kwargs):\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
    "        if isinstance(padding, int):\n",
    "            self.padding = ((padding, padding), (padding, padding))\n",
    "        elif hasattr(padding, '__len__'):\n",
    "            if len(padding) != 2:\n",
    "                raise ValueError('`padding` should have two elements. '\n",
    "                                 'Found: ' + str(padding))\n",
    "            height_padding = conv_utils.normalize_tuple(padding[0], 2,\n",
    "                                                        '1st entry of padding')\n",
    "            width_padding = conv_utils.normalize_tuple(padding[1], 2,\n",
    "                                                       '2nd entry of padding')\n",
    "            self.padding = (height_padding, width_padding)\n",
    "        else:\n",
    "            raise ValueError('`padding` should be either an int, '\n",
    "                             'a tuple of 2 ints '\n",
    "                             '(symmetric_height_pad, symmetric_width_pad), '\n",
    "                             'or a tuple of 2 tuples of 2 ints '\n",
    "                             '((top_pad, bottom_pad), (left_pad, right_pad)). '\n",
    "                             'Found: ' + str(padding))\n",
    "        self.input_spec = InputSpec(ndim=4)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            if input_shape[2] is not None:\n",
    "                rows = input_shape[2] + self.padding[0][0] + self.padding[0][1]\n",
    "            else:\n",
    "                rows = None\n",
    "            if input_shape[3] is not None:\n",
    "                cols = input_shape[3] + self.padding[1][0] + self.padding[1][1]\n",
    "            else:\n",
    "                cols = None\n",
    "            return (input_shape[0],\n",
    "                    input_shape[1],\n",
    "                    rows,\n",
    "                    cols)\n",
    "        elif self.data_format == 'channels_last':\n",
    "            if input_shape[1] is not None:\n",
    "                rows = input_shape[1] + self.padding[0][0] + self.padding[0][1]\n",
    "            else:\n",
    "                rows = None\n",
    "            if input_shape[2] is not None:\n",
    "                cols = input_shape[2] + self.padding[1][0] + self.padding[1][1]\n",
    "            else:\n",
    "                cols = None\n",
    "            return (input_shape[0],\n",
    "                    rows,\n",
    "                    cols,\n",
    "                    input_shape[3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return spatial_reflection_2d_padding(inputs,\n",
    "                                             padding=self.padding,\n",
    "                                             data_format=self.data_format)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'padding': self.padding,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(ReflectionPadding2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Activation, Add\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "ngf = 64\n",
    "input_nc = 3\n",
    "output_nc = 3\n",
    "input_shape_generator = (256, 256, input_nc)\n",
    "n_blocks_gen = 9\n",
    "\n",
    "\n",
    "def generator_model():\n",
    "    \"\"\"Build generator architecture.\"\"\"\n",
    "    # Current version : ResNet block\n",
    "    inputs = Input(shape=image_shape)\n",
    "\n",
    "    x = ReflectionPadding2D((3, 3))(inputs)\n",
    "    x = Conv2D(filters=ngf, kernel_size=(7,7), padding='valid')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Increase filter number\n",
    "    n_downsampling = 2\n",
    "    for i in range(n_downsampling):\n",
    "        mult = 2**i\n",
    "        x = Conv2D(filters=ngf*mult*2, kernel_size=(3,3), strides=2, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "    # Apply 9 ResNet blocks\n",
    "    mult = 2**n_downsampling\n",
    "    for i in range(n_blocks_gen):\n",
    "        x = res_block(x, ngf*mult, use_dropout=True)\n",
    "\n",
    "    # Decrease filter number to 3 (RGB)\n",
    "    for i in range(n_downsampling):\n",
    "        mult = 2**(n_downsampling - i)\n",
    "        x = Conv2DTranspose(filters=int(ngf * mult / 2), kernel_size=(3,3), strides=2, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "    x = ReflectionPadding2D((3,3))(x)\n",
    "    x = Conv2D(filters=output_nc, kernel_size=(7,7), padding='valid')(x)\n",
    "    x = Activation('tanh')(x)\n",
    "\n",
    "    # Add direct connection from input to output and recenter to [-1, 1]\n",
    "    outputs = Add()([x, inputs])\n",
    "    outputs = Lambda(lambda z: z/2)(outputs)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Generator')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "ndf = 64\n",
    "output_nc = 3\n",
    "input_shape_discriminator = (256, 256, output_nc)\n",
    "\n",
    "\n",
    "def discriminator_model():\n",
    "    \"\"\"Build discriminator architecture.\"\"\"\n",
    "    n_layers, use_sigmoid = 3, False\n",
    "    inputs = Input(shape=input_shape_discriminator)\n",
    "\n",
    "    x = Conv2D(filters=ndf, kernel_size=(4,4), strides=2, padding='same')(inputs)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    nf_mult, nf_mult_prev = 1, 1\n",
    "    for n in range(n_layers):\n",
    "        nf_mult_prev, nf_mult = nf_mult, min(2**n, 8)\n",
    "        x = Conv2D(filters=ndf*nf_mult, kernel_size=(4,4), strides=2, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    nf_mult_prev, nf_mult = nf_mult, min(2**n_layers, 8)\n",
    "    x = Conv2D(filters=ndf*nf_mult, kernel_size=(4,4), strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = Conv2D(filters=1, kernel_size=(4,4), strides=1, padding='same')(x)\n",
    "    if use_sigmoid:\n",
    "        x = Activation('sigmoid')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='tanh')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x, name='Discriminator')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "def generator_containing_discriminator_multiple_outputs(generator, discriminator):\n",
    "    inputs = Input(shape=image_shape)\n",
    "    generated_images = generator(inputs)\n",
    "    outputs = discriminator(generated_images)\n",
    "    model = Model(inputs=inputs, outputs=[generated_images, outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "\n",
    "image_shape = (256, 256, 3)\n",
    "\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    vgg = VGG16(include_top=False, weights='imagenet', input_shape=image_shape)\n",
    "    loss_model = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n",
    "    loss_model.trainable = False\n",
    "    return K.mean(K.square(loss_model(y_true) - loss_model(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true*y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/PRNU/data/imgs\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/PRNU/data/imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imutils\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(370, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "img_list = os.listdir('.')\n",
    "t = cv2.imread(img_list[0])\n",
    "t = (t-128)/128\n",
    "imgs = np.array([t])\n",
    "for i in img_list:\n",
    "    t = cv2.imread(i)\n",
    "    if t.shape[0]==imgs.shape[2] or t.shape[0]==imgs.shape[1]:\n",
    "        pass\n",
    "    else:\n",
    "        continue\n",
    "    if t.shape[0]==imgs.shape[2]:\n",
    "        t = imutils.rotate_bound(t,90)\n",
    "    t = (t-128)/128\n",
    "    imgs = np.append(imgs,[t],axis=0)\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/PRNU/data/prnu\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/PRNU/data/prnu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "prnu_list = os.listdir('.')\n",
    "prnus = np.array([np.load(prnu_list[0])])\n",
    "for i in prnu_list[1:]:\n",
    "    t = np.load(i)\n",
    "    prnus = np.append(prnus,[t],axis=0)\n",
    "print(prnus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/PRNU/data\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/PRNU/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "g = generator_model()\n",
    "d = discriminator_model()\n",
    "d_on_g = generator_containing_discriminator_multiple_outputs(g, d)\n",
    "\n",
    "g_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "d_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "d_on_g_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "d.trainable = True\n",
    "d.compile(optimizer=d_opt, loss=wasserstein_loss)\n",
    "d.trainable = False\n",
    "loss = [perceptual_loss, wasserstein_loss]\n",
    "loss_weights = [100, 1]\n",
    "d_on_g.compile(optimizer=d_on_g_opt, loss=loss, loss_weights=loss_weights)\n",
    "d.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_all_weights(d, g, epoch_number, current_loss):\n",
    "    now = datetime.datetime.now()\n",
    "    save_dir = os.path.join(BASE_DIR, '{}{}'.format(now.month, now.day))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    g.save_weights(os.path.join(save_dir, 'generator_{}_{}.h5'.format(epoch_number, current_loss)), True)\n",
    "    d.save_weights(os.path.join(save_dir, 'discriminator_{}.h5'.format(epoch_number)), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/50\n",
      "batches: 35.0\n",
      "batch 1 d_loss : 0.06513088489882649\n",
      "batch 1 d_on_g_loss : [11377.392, 113.7735, 0.04245216]\n",
      "batch 2 d_loss : 0.03484101839712821\n",
      "batch 2 d_on_g_loss : [13551.083, 135.5108, 0.0027434381]\n",
      "batch 3 d_loss : 0.023456811156938782\n",
      "batch 3 d_on_g_loss : [13455.451, 134.5545, 0.00066995644]\n",
      "batch 4 d_loss : 0.01765426189012942\n",
      "batch 4 d_on_g_loss : [11159.271, 111.59271, 0.00038178678]\n",
      "batch 5 d_loss : 0.014142265522677918\n",
      "batch 5 d_on_g_loss : [12518.078, 125.18079, 0.00025217299]\n",
      "batch 6 d_loss : 0.011791872220783262\n",
      "batch 6 d_on_g_loss : [8194.3428, 81.943428, 0.00024214009]\n",
      "batch 7 d_loss : 0.010111300535676751\n",
      "batch 7 d_on_g_loss : [10699.355, 106.99355, 0.00011582988]\n",
      "batch 8 d_loss : 0.00885150073913792\n",
      "batch 8 d_on_g_loss : [11651.772, 116.51772, 0.00015007294]\n",
      "batch 9 d_loss : 0.007870201265500833\n",
      "batch 9 d_on_g_loss : [13688.965, 136.88965, 0.00013616479]\n",
      "batch 10 d_loss : 0.007086519736258196\n",
      "batch 10 d_on_g_loss : [14374.701, 143.74701, 0.0001399723]\n",
      "batch 11 d_loss : 0.006444568006058531\n",
      "batch 11 d_on_g_loss : [12301.553, 123.01553, 0.00011797623]\n",
      "batch 12 d_loss : 0.005908867122313192\n",
      "batch 12 d_on_g_loss : [9796.9941, 97.96994, 9.7510383e-05]\n",
      "batch 13 d_loss : 0.005457108464711032\n",
      "batch 13 d_on_g_loss : [11003.14, 110.03139, 9.990696e-05]\n",
      "batch 14 d_loss : 0.00506869015249062\n",
      "batch 14 d_on_g_loss : [8983.708, 89.837082, 9.4292845e-05]\n",
      "batch 15 d_loss : 0.004731553701155159\n",
      "batch 15 d_on_g_loss : [12527.546, 125.27546, 8.4433676e-05]\n",
      "batch 16 d_loss : 0.004437548263194912\n",
      "batch 16 d_on_g_loss : [10140.586, 101.40586, 8.1455408e-05]\n",
      "batch 17 d_loss : 0.004177044729722718\n",
      "batch 17 d_on_g_loss : [13121.11, 131.21111, 6.7250912e-05]\n",
      "batch 18 d_loss : 0.003945937792438296\n",
      "batch 18 d_on_g_loss : [12977.129, 129.77129, 8.7763154e-05]\n",
      "batch 19 d_loss : 0.003739191682478505\n",
      "batch 19 d_on_g_loss : [12264.758, 122.64758, 7.3234347e-05]\n",
      "batch 20 d_loss : 0.0035525709661169457\n",
      "batch 20 d_on_g_loss : [11119.362, 111.19363, 8.1316139e-05]\n",
      "batch 21 d_loss : 0.0033839333921345575\n",
      "batch 21 d_on_g_loss : [10199.259, 101.99258, 6.9959278e-05]\n",
      "batch 22 d_loss : 0.003230581220135388\n",
      "batch 22 d_on_g_loss : [12254.783, 122.54783, 6.1062965e-05]\n",
      "batch 23 d_loss : 0.0030909549479988725\n",
      "batch 23 d_on_g_loss : [13638.577, 136.38577, 4.9216487e-05]\n",
      "batch 24 d_loss : 0.002962564214495463\n",
      "batch 24 d_on_g_loss : [10973.865, 109.73865, 9.3022762e-05]\n",
      "batch 25 d_loss : 0.0028444521716846794\n",
      "batch 25 d_on_g_loss : [13467.172, 134.67172, 6.7416455e-05]\n",
      "batch 26 d_loss : 0.0027353609869456147\n",
      "batch 26 d_on_g_loss : [14556.438, 145.56438, 3.9778366e-05]\n",
      "batch 27 d_loss : 0.0026345545324855065\n",
      "batch 27 d_on_g_loss : [12228.942, 122.28942, 5.2313288e-05]\n",
      "batch 28 d_loss : 0.0025407191221672423\n",
      "batch 28 d_on_g_loss : [12146.671, 121.46671, 4.5253095e-05]\n",
      "batch 29 d_loss : 0.0024533658087891345\n",
      "batch 29 d_on_g_loss : [14061.682, 140.61682, 3.9733299e-05]\n",
      "batch 30 d_loss : 0.0023718347329243746\n",
      "batch 30 d_on_g_loss : [13356.337, 133.56337, 8.8565015e-05]\n",
      "batch 31 d_loss : 0.002295568952497213\n",
      "batch 31 d_on_g_loss : [12164.315, 121.64316, 7.0590664e-05]\n",
      "batch 32 d_loss : 0.002224024740024788\n",
      "batch 32 d_on_g_loss : [9900.9482, 99.009483, 4.5779001e-05]\n",
      "batch 33 d_loss : 0.0021567636706453356\n",
      "batch 33 d_on_g_loss : [12030.448, 120.30448, 6.7247718e-05]\n",
      "batch 34 d_loss : 0.0020935214296915706\n",
      "batch 34 d_on_g_loss : [11369.414, 113.69415, 3.8647759e-05]\n",
      "batch 35 d_loss : 0.002033808252059056\n",
      "batch 35 d_on_g_loss : [10390.311, 103.90311, 4.5790694e-05]\n",
      "epoch: 1/50\n",
      "batches: 35.0\n",
      "batch 1 d_loss : 7.941519834275823e-06\n",
      "batch 1 d_on_g_loss : [14255.136, 142.55136, 3.9715946e-05]\n",
      "batch 2 d_loss : 6.916856045791065e-06\n",
      "batch 2 d_on_g_loss : [11335.924, 113.35924, 4.7835372e-05]\n",
      "batch 3 d_loss : 6.0955117078265175e-06\n",
      "batch 3 d_on_g_loss : [12331.308, 123.31308, 4.651929e-05]\n",
      "batch 4 d_loss : 6.281002265495772e-06\n",
      "batch 4 d_on_g_loss : [16675.311, 166.75311, 6.3851418e-05]\n",
      "batch 5 d_loss : 6.25528477030457e-06\n",
      "batch 5 d_on_g_loss : [11274.063, 112.74064, 4.2474392e-05]\n",
      "batch 6 d_loss : 5.783125440454266e-06\n",
      "batch 6 d_on_g_loss : [10795.941, 107.95941, 6.5031491e-05]\n",
      "batch 7 d_loss : 5.57143970841675e-06\n",
      "batch 7 d_on_g_loss : [10076.324, 100.76324, 4.5888424e-05]\n",
      "batch 8 d_loss : 5.542036797123729e-06\n",
      "batch 8 d_on_g_loss : [12471.727, 124.71727, 3.8362239e-05]\n",
      "batch 9 d_loss : 5.493915917516764e-06\n",
      "batch 9 d_on_g_loss : [11423.685, 114.23685, 3.4359247e-05]\n",
      "batch 10 d_loss : 5.358157932278118e-06\n",
      "batch 10 d_on_g_loss : [10906.812, 109.06812, 4.1802545e-05]\n",
      "batch 11 d_loss : 5.093907694489727e-06\n",
      "batch 11 d_on_g_loss : [12529.106, 125.29106, 4.427231e-05]\n",
      "batch 12 d_loss : 4.956537141727798e-06\n",
      "batch 12 d_on_g_loss : [11224.167, 112.24167, 4.768894e-05]\n",
      "batch 13 d_loss : 4.85772376613078e-06\n",
      "batch 13 d_on_g_loss : [10753.578, 107.53578, 5.2896263e-05]\n",
      "batch 14 d_loss : 4.747780326397333e-06\n",
      "batch 14 d_on_g_loss : [9926.1162, 99.261162, 4.4865912e-05]\n",
      "batch 15 d_loss : 4.738354488533029e-06\n",
      "batch 15 d_on_g_loss : [14947.435, 149.47435, 4.8030564e-05]\n",
      "batch 16 d_loss : 4.613406778730677e-06\n",
      "batch 16 d_on_g_loss : [12978.969, 129.78969, 3.4663048e-05]\n",
      "batch 17 d_loss : 4.572087311102865e-06\n",
      "batch 17 d_on_g_loss : [9481.9717, 94.819717, 3.804209e-05]\n",
      "batch 18 d_loss : 4.484520412814972e-06\n",
      "batch 18 d_on_g_loss : [10765.055, 107.65055, 2.469285e-05]\n",
      "batch 19 d_loss : 4.454019902990886e-06\n",
      "batch 19 d_on_g_loss : [10923.457, 109.23457, 3.6013844e-05]\n",
      "batch 20 d_loss : 4.3975571475129984e-06\n",
      "batch 20 d_on_g_loss : [12491.833, 124.91833, 3.3951554e-05]\n",
      "batch 21 d_loss : 4.2864764769287184e-06\n",
      "batch 21 d_on_g_loss : [8811.5195, 88.115196, 3.2308031e-05]\n",
      "batch 22 d_loss : 4.3524581955849e-06\n",
      "batch 22 d_on_g_loss : [12757.923, 127.57923, 3.4204437e-05]\n",
      "batch 23 d_loss : 4.271355704116539e-06\n",
      "batch 23 d_on_g_loss : [12643.696, 126.43696, 1.9423656e-05]\n",
      "batch 24 d_loss : 4.22213967302317e-06\n",
      "batch 24 d_on_g_loss : [10805.978, 108.05978, 3.0929812e-05]\n",
      "batch 25 d_loss : 4.170288075329154e-06\n",
      "batch 25 d_on_g_loss : [13340.592, 133.40591, 3.7263875e-05]\n",
      "batch 26 d_loss : 4.138195726437306e-06\n",
      "batch 26 d_on_g_loss : [12132.27, 121.32269, 4.5890083e-05]\n",
      "batch 27 d_loss : 4.098234175789575e-06\n",
      "batch 27 d_on_g_loss : [13466.258, 134.66258, 4.9175043e-05]\n",
      "batch 28 d_loss : 4.077799782667821e-06\n",
      "batch 28 d_on_g_loss : [8764.8145, 87.64814, 2.7192193e-05]\n",
      "batch 29 d_loss : 4.058815742108232e-06\n",
      "batch 29 d_on_g_loss : [9744.0518, 97.440521, 2.6833059e-05]\n",
      "batch 30 d_loss : 4.050515591795071e-06\n",
      "batch 30 d_on_g_loss : [12925.25, 129.2525, 2.412277e-05]\n",
      "batch 31 d_loss : 4.02574755606756e-06\n",
      "batch 31 d_on_g_loss : [12383.671, 123.83671, 2.7822056e-05]\n",
      "batch 32 d_loss : 3.950827248644373e-06\n",
      "batch 32 d_on_g_loss : [9299.2578, 92.992577, 2.6326486e-05]\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 50\n",
    "batch_size = 8\n",
    "critic_updates = 5\n",
    "BASE_DIR = 'weights/'\n",
    "x_train = prnus.copy()\n",
    "y_train = imgs.copy()\n",
    "output_true_batch, output_false_batch = np.ones((batch_size, 1)), np.zeros((batch_size, 1))\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    print('epoch: {}/{}'.format(epoch, epoch_num))\n",
    "    print('batches: {}'.format(x_train.shape[0] / batch_size))\n",
    "    \n",
    "    permutated_indexes = np.random.permutation(x_train.shape[0])\n",
    "    \n",
    "    d_losses = []\n",
    "    d_on_g_losses = []\n",
    "    \n",
    "    for index in range(int(x_train.shape[0] / batch_size)):\n",
    "        batch_indexes = permutated_indexes[index*batch_size:(index+1)*batch_size]\n",
    "        image_blur_batch = x_train[batch_indexes]\n",
    "        image_full_batch = y_train[batch_indexes]\n",
    "        \n",
    "        generated_images = g.predict(x=image_blur_batch, batch_size=batch_size)\n",
    "        \n",
    "        for _ in range(critic_updates):\n",
    "            d_loss_real = d.train_on_batch(image_full_batch, output_true_batch)\n",
    "            d_loss_fake = d.train_on_batch(generated_images, output_false_batch)\n",
    "            d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "            d_losses.append(d_loss)\n",
    "        print('batch {} d_loss : {}'.format(index+1, np.mean(d_losses)))\n",
    "        \n",
    "        d.trainable = False\n",
    "        \n",
    "        d_on_g_loss = d_on_g.train_on_batch(image_blur_batch, [image_full_batch, output_true_batch])\n",
    "        d_on_g_losses.append(d_on_g_loss)\n",
    "        print('batch {} d_on_g_loss : {}'.format(index+1, d_on_g_loss))\n",
    "        \n",
    "        d.trainable = True\n",
    "        \n",
    "    with open('log.txt', 'a') as f:\n",
    "        f.write('{} - {} - {}\\n'.format(epoch, np.mean(d_losses), np.mean(d_on_g_losses)))\n",
    "    \n",
    "    save_all_weights(d, g, epoch, int(np.mean(d_on_g_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/PRNU/data/imgs\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/PRNU/data/imgs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 256, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tt = cv2.imread('Agfa_DC-733s_01.jpg')\n",
    "x_tt = (x_tt-128)/128\n",
    "x_tt = np.array([x_tt])\n",
    "x_tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/PRNU/data/weights/712\n"
     ]
    }
   ],
   "source": [
    "cd ../weights/712/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = generator_model()\n",
    "g.load_weights('generator_9_3729.h5')\n",
    "generated_images = g.predict(x=x_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('aa.jpg',generated_images[0]*255)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4cv",
   "language": "python",
   "name": "dl4cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
